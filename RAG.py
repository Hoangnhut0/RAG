import streamlit as st
import fitz  # PyMuPDF Ä‘á»ƒ trÃ­ch xuáº¥t vÄƒn báº£n tá»« PDF
import pytesseract
from pdf2image import convert_from_path
import qdrant_client
from qdrant_client.models import PointStruct, Distance, VectorParams
import os
import uuid
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
import cv2  # OpenCV Ä‘á»ƒ xá»­ lÃ½ áº£nh
from PIL import Image  # Ä‘á»ƒ xá»­ lÃ½ áº£nh
from fuzzywuzzy import fuzz  # So sÃ¡nh chuá»—i gáº§n Ä‘Ãºng
import nltk  # Thay tháº¿ spaCy Ä‘á»ƒ xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn
import numpy as np

# Táº£i tÃ i nguyÃªn punkt_tab náº¿u chÆ°a cÃ³
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)

# ğŸ”¹ Cáº¥u hÃ¬nh Gemini API
genai.configure(api_key="AIzaSyCc5UqNQ1IrAN5rwLQQc4WTckKV0KRfgKw")

# ğŸ”¹ Khá»Ÿi táº¡o mÃ´ hÃ¬nh embedding
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# ğŸ”¹ Káº¿t ná»‘i Ä‘áº¿n Qdrant
qdrant = qdrant_client.QdrantClient("localhost", port=6333)
collection_name = "library_docs"

def ensure_collection_exists():
    collections = qdrant.get_collections()
    collection_names = [col.name for col in collections.collections]
    if collection_name not in collection_names:
        qdrant.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=384, distance=Distance.COSINE),
        )
        st.success(f"âœ… Collection `{collection_name}` Ä‘Ã£ Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng!")

ensure_collection_exists()

# Xá»­ lÃ½ hÃ¬nh áº£nh trÆ°á»›c OCR
def preprocess_image(image):
    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)
    gray = cv2.convertScaleAbs(gray, alpha=1.5, beta=0)
    gray = cv2.fastNlMeansDenoising(gray, h=10)
    return Image.fromarray(gray)

# Sá»­a lá»—i chÃ­nh táº£ báº±ng Gemini AI
def correct_spelling_with_gemini(text):
    model = genai.GenerativeModel("gemini-2.0-flash")
    prompt = f"""
    VÄƒn báº£n sau cÃ³ thá»ƒ chá»©a lá»—i chÃ­nh táº£ tiáº¿ng Viá»‡t vÃ  tiáº¿ng Anh do trÃ­ch xuáº¥t tá»« OCR.
    HÃ£y sá»­a lá»—i chÃ­nh táº£ vÃ  tráº£ vá» vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c chá»‰nh sá»­a:
    "{text}"
    """
    try:
        response = model.generate_content(prompt)
        corrected_text = response.text.strip() if response else text
        return corrected_text
    except Exception as e:
        print(f"âŒ Lá»—i khi sá»­a chÃ­nh táº£ vá»›i Gemini: {e}")
        return text  

# ğŸ“„ TrÃ­ch xuáº¥t vÄƒn báº£n tá»« PDF thÆ°á»ng vÃ  PDF scan
def extract_text_from_pdf(pdf_path):
    try:
        doc = fitz.open(pdf_path)
        text_content = []
        
        for page_num, page in enumerate(doc, start=1):
            text = page.get_text("text").strip()
            if text:
                text_content.append(f"{text}\n[Trang {page_num}]")
        
        if text_content:
            extracted_text = "\n".join(text_content)
            return extracted_text
        
        return extract_text_with_ocr(pdf_path)
    except Exception as e:
        st.error(f"âŒ Lá»—i khi Ä‘á»c PDF: {e}")
        return "KhÃ´ng cÃ³ ná»™i dung."

# Sá»­ dá»¥ng OCR vá»›i tiá»n xá»­ lÃ½ nÃ¢ng cao vÃ  sá»­a lá»—i chÃ­nh táº£
def extract_text_with_ocr(pdf_path):
    try:
        images = convert_from_path(pdf_path)
        text_content = []
        
        for i, img in enumerate(images):
            processed_img = preprocess_image(img)
            custom_config = r'--oem 3 --psm 6 -l eng+vie'
            raw_text = pytesseract.image_to_string(processed_img, config=custom_config).strip()
            if raw_text:
                corrected_text = correct_spelling_with_gemini(raw_text)
                text_content.append(f"{corrected_text}\n[Trang {i+1}]")
                print(f"ğŸ“œ VÄƒn báº£n sau khi sá»­a lá»—i chÃ­nh táº£ (Trang {i+1}):\n{corrected_text}\n{'='*50}")
        
        extracted_text = "\n".join(text_content) if text_content else "KhÃ´ng cÃ³ ná»™i dung."
        return extracted_text
    except Exception as e:
        st.error(f"âŒ Lá»—i khi xá»­ lÃ½ OCR: {e}")
        return "KhÃ´ng cÃ³ ná»™i dung."

# ğŸ”¹ Chuyá»ƒn vÄƒn báº£n thÃ nh vector embedding
def generate_embedding(text):
    return embedding_model.encode(text).tolist() if text.strip() else [0.0] * 384

# ğŸ”¹ LÆ°u tÃ i liá»‡u vÃ o Qdrant
def store_document_in_qdrant(doc_id, text, metadata):
    vector = generate_embedding(text)
    metadata["text_content"] = text
    print(f"ğŸ“¦ VÄƒn báº£n cuá»‘i cÃ¹ng lÆ°u vÃ o Qdrant:\n{text}\n{'='*50}")
    qdrant.upsert(
        collection_name=collection_name,
        points=[PointStruct(id=doc_id, vector=vector, payload=metadata)]
    )

# ğŸ” TÃ¬m kiáº¿m tÃ i liá»‡u trong Qdrant vá»›i Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cáº£i thiá»‡n
def search_documents(query):
    query_vector = generate_embedding(query)
    results = qdrant.search(
        collection_name=collection_name,
        query_vector=query_vector,
        limit=10,
        with_payload=True
    )
    filtered_results = [r for r in results if r.score >= 0.5]
    return filtered_results

# ğŸ“Œ TrÃ­ch xuáº¥t Ä‘oáº¡n vÄƒn báº£n vá»›i nltk vÃ  tÃ¬m sá»‘ trang chÃ­nh xÃ¡c hÆ¡n
def extract_relevant_text(full_text, query, filename, context_size=300):
    sentences = nltk.sent_tokenize(full_text)
    query_embedding = generate_embedding(query)
    
    best_sentence = None
    best_similarity = -1
    best_sentence_idx = -1
    
    for idx, sent in enumerate(sentences):
        sent_embedding = generate_embedding(sent)
        similarity = np.dot(query_embedding, sent_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(sent_embedding))
        if similarity > best_similarity:
            best_similarity = similarity
            best_sentence = sent
            best_sentence_idx = idx
    
    if best_sentence and best_similarity > 0.7:
        paragraphs = full_text.split("\n\n")
        relevant_paragraph = None
        
        for para in paragraphs:
            if best_sentence in para:
                relevant_paragraph = para
                break
        
        if relevant_paragraph:
            para_sentences = nltk.sent_tokenize(relevant_paragraph)
            if len(para_sentences) < 3:
                para_idx = paragraphs.index(relevant_paragraph)
                start_para_idx = max(0, para_idx - 1)
                end_para_idx = min(len(paragraphs), para_idx + 2)
                relevant_text = "\n\n".join(paragraphs[start_para_idx:end_para_idx]).strip()
            else:
                relevant_text = relevant_paragraph.strip()
            
            page_number = "KhÃ´ng rÃµ trang"
            text_before = full_text[:full_text.find(relevant_text) + len(relevant_text)]
            pages = [line for line in text_before.split("\n") if "[Trang " in line]
            if pages:
                last_page_line = pages[-1]
                page_number = last_page_line.split("[Trang ")[-1].replace("]", "").strip()
            
            return f"{relevant_text}...", page_number
        
        relevant_text = determine_paragraph_boundaries_with_gemini(full_text, best_sentence)
        if relevant_text:
            page_number = "KhÃ´ng rÃµ trang"
            text_before = full_text[:full_text.find(relevant_text) + len(relevant_text)]
            pages = [line for line in text_before.split("\n") if "[Trang " in line]
            if pages:
                last_page_line = pages[-1]
                page_number = last_page_line.split("[Trang ")[-1].replace("]", "").strip()
            return f"{relevant_text}...", page_number
        
        start_idx = full_text.find(best_sentence)
        if start_idx == -1:
            return None, "KhÃ´ng rÃµ trang"
        
        start = max(0, full_text.rfind("\n\n", 0, start_idx))
        end = full_text.find("\n\n", start_idx + len(best_sentence))
        if end == -1:
            end = len(full_text)
        relevant_text = full_text[start:end].strip()
        
        page_number = "KhÃ´ng rÃµ trang"
        text_before = full_text[:end]
        pages = [line for line in text_before.split("\n") if "[Trang " in line]
        if pages:
            last_page_line = pages[-1]
            page_number = last_page_line.split("[Trang ")[-1].replace("]", "").strip()
        
        return f"{relevant_text}...", page_number
    
    return None, "KhÃ´ng rÃµ trang"

# Sá»­ dá»¥ng Gemini AI Ä‘á»ƒ xÃ¡c Ä‘á»‹nh ranh giá»›i Ä‘oáº¡n vÄƒn
def determine_paragraph_boundaries_with_gemini(full_text, best_sentence):
    model = genai.GenerativeModel("gemini-2.0-flash")
    prompt = f"""
    DÆ°á»›i Ä‘Ã¢y lÃ  má»™t Ä‘oáº¡n vÄƒn báº£n dÃ i:
    "{full_text[:2000]}"
    
    CÃ¢u sau náº±m trong vÄƒn báº£n: "{best_sentence}"
    HÃ£y xÃ¡c Ä‘á»‹nh Ä‘oáº¡n vÄƒn (paragraph) chá»©a cÃ¢u nÃ y vÃ  tráº£ vá» Ä‘oáº¡n vÄƒn Ä‘Ã³.
    """
    try:
        response = model.generate_content(prompt)
        relevant_paragraph = response.text.strip() if response else best_sentence
        return relevant_paragraph
    except Exception as e:
        print(f"âŒ Lá»—i khi xÃ¡c Ä‘á»‹nh Ä‘oáº¡n vÄƒn vá»›i Gemini: {e}")
        return best_sentence

# ğŸ¤– Kiá»ƒm tra má»©c Ä‘á»™ liÃªn quan cá»§a ná»™i dung trÃ­ch xuáº¥t vá»›i cÃ¢u há»i
def check_relevance_with_gemini(query, relevant_text):
    model = genai.GenerativeModel("gemini-2.0-flash")
    prompt = f"""
    CÃ¢u há»i: "{query}"
    Ná»™i dung trÃ­ch xuáº¥t tá»« tÃ i liá»‡u: "{relevant_text}"
    HÃ£y Ä‘Ã¡nh giÃ¡ xem ná»™i dung trÃ­ch xuáº¥t cÃ³ thá»±c sá»± liÃªn quan Ä‘áº¿n cÃ¢u há»i khÃ´ng.
    Tráº£ vá» "CÃ³" náº¿u liÃªn quan, "KhÃ´ng" náº¿u khÃ´ng liÃªn quan.
    """
    try:
        response = model.generate_content(prompt)
        result = response.text.strip() if response else "KhÃ´ng"
        return result == "CÃ³"
    except Exception as e:
        print(f"âŒ Lá»—i khi kiá»ƒm tra má»©c Ä‘á»™ liÃªn quan vá»›i Gemini: {e}")
        return False

# ğŸ¤– Xá»­ lÃ½ Gemini AI vá»›i prompt cáº£i tiáº¿n
def process_with_gemini(query, filename=None, context=None, full_text=None):
    model = genai.GenerativeModel("gemini-2.0-flash")
    
    if context and full_text:
        # Prompt cáº£i tiáº¿n Ä‘á»ƒ Gemini hiá»ƒu cÃ¢u há»i vÃ  tÃ¬m kiáº¿m trong tÃ i liá»‡u
        prompt = f"""
        Báº¡n lÃ  má»™t trá»£ lÃ½ thÃ´ng minh giÃºp tráº£ lá»i cÃ¢u há»i dá»±a trÃªn ná»™i dung tÃ i liá»‡u.
        
        **CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng:** "{query}"
        **TÃ i liá»‡u liÃªn quan:** `{filename}`
        **Ná»™i dung trÃ­ch xuáº¥t ban Ä‘áº§u tá»« tÃ i liá»‡u (cÃ³ thá»ƒ khÃ´ng Ä‘áº§y Ä‘á»§):** 
        {context}
        
        **ToÃ n bá»™ ná»™i dung tÃ i liá»‡u (dÃ¹ng Ä‘á»ƒ tÃ¬m kiáº¿m thÃªm náº¿u cáº§n):** 
        {full_text[:5000]}  # Giá»›i háº¡n Ä‘á»™ dÃ i Ä‘á»ƒ trÃ¡nh vÆ°á»£t quÃ¡ giá»›i háº¡n cá»§a Gemini
        
        **Nhiá»‡m vá»¥:**
        1. Hiá»ƒu cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng vÃ  xÃ¡c Ä‘á»‹nh thÃ´ng tin cáº§n thiáº¿t Ä‘á»ƒ tráº£ lá»i.
        2. TÃ¬m kiáº¿m trong toÃ n bá»™ ná»™i dung tÃ i liá»‡u Ä‘á»ƒ láº¥y Ä‘oáº¡n vÄƒn báº£n liÃªn quan nháº¥t Ä‘áº¿n cÃ¢u há»i.
        3. Tráº£ lá»i cÃ¢u há»i má»™t cÃ¡ch chÃ­nh xÃ¡c, dá»… hiá»ƒu, vÃ  dá»±a trÃªn thÃ´ng tin trong tÃ i liá»‡u.
        4. Náº¿u thÃ´ng tin trong tÃ i liá»‡u khÃ´ng Ä‘á»§, hÃ£y giáº£i thÃ­ch ngáº¯n gá»n vÃ  Ä‘Æ°a ra gá»£i Ã½ há»£p lÃ½.
        
        **Äá»‹nh dáº¡ng tráº£ lá»i:**
        - Náº¿u tÃ¬m tháº¥y thÃ´ng tin liÃªn quan: Tráº£ lá»i cÃ¢u há»i vÃ  trÃ­ch dáº«n Ä‘oáº¡n vÄƒn báº£n liÃªn quan tá»« tÃ i liá»‡u.
        - Náº¿u khÃ´ng tÃ¬m tháº¥y: Giáº£i thÃ­ch ráº±ng khÃ´ng cÃ³ thÃ´ng tin phÃ¹ há»£p vÃ  Ä‘Æ°a ra gá»£i Ã½.
        """
    elif context:
        prompt = f"""
        Báº¡n lÃ  má»™t trá»£ lÃ½ thÃ´ng minh giÃºp tráº£ lá»i cÃ¢u há»i dá»±a trÃªn ná»™i dung tÃ i liá»‡u.
        
        **CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng:** "{query}"
        **TÃ i liá»‡u liÃªn quan:** `{filename}`
        **Ná»™i dung trÃ­ch xuáº¥t tá»« tÃ i liá»‡u:** 
        {context}
        
        **Nhiá»‡m vá»¥:**
        1. Hiá»ƒu cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng vÃ  xÃ¡c Ä‘á»‹nh thÃ´ng tin cáº§n thiáº¿t Ä‘á»ƒ tráº£ lá»i.
        2. Dá»±a vÃ o ná»™i dung trÃ­ch xuáº¥t Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i má»™t cÃ¡ch chÃ­nh xÃ¡c vÃ  dá»… hiá»ƒu.
        3. Náº¿u thÃ´ng tin khÃ´ng Ä‘á»§, hÃ£y giáº£i thÃ­ch ngáº¯n gá»n vÃ  Ä‘Æ°a ra gá»£i Ã½ há»£p lÃ½.
        
        **Äá»‹nh dáº¡ng tráº£ lá»i:**
        - Tráº£ lá»i cÃ¢u há»i vÃ  trÃ­ch dáº«n Ä‘oáº¡n vÄƒn báº£n liÃªn quan tá»« ná»™i dung trÃ­ch xuáº¥t.
        - Náº¿u khÃ´ng Ä‘á»§ thÃ´ng tin, giáº£i thÃ­ch vÃ  Ä‘Æ°a ra gá»£i Ã½.
        """
    else:
        prompt = f"""
        Báº¡n lÃ  má»™t trá»£ lÃ½ thÃ´ng minh giÃºp tráº£ lá»i cÃ¢u há»i.
        
        **CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng:** "{query}"
        **ThÃ´ng tin:** KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u phÃ¹ há»£p.
        
        **Nhiá»‡m vá»¥:**
        1. Tráº£ lá»i cÃ¢u há»i dá»±a trÃªn kiáº¿n thá»©c chung cá»§a báº¡n má»™t cÃ¡ch ngáº¯n gá»n vÃ  dá»… hiá»ƒu.
        2. Náº¿u khÃ´ng thá»ƒ tráº£ lá»i, hÃ£y giáº£i thÃ­ch vÃ  Ä‘Æ°a ra gá»£i Ã½ há»£p lÃ½.
        """
    
    try:
        response = model.generate_content(prompt)
        return response.text if response else "KhÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i."
    except Exception as e:
        print(f"âŒ Lá»—i khi xá»­ lÃ½ vá»›i Gemini: {e}")
        return "KhÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i."

# ğŸ¯ Giao diá»‡n Streamlit
def main():
    st.title("ğŸ“š ThÆ° viá»‡n sá»‘ thÃ´ng minh")
    
    uploaded_file = st.file_uploader("ğŸ“¤ Táº£i lÃªn tÃ i liá»‡u PDF", type=["pdf"])
    if uploaded_file and st.button("ğŸ“¥ LÆ°u tÃ i liá»‡u"):
        pdf_path = f"uploads/{uploaded_file.name}"
        os.makedirs("uploads", exist_ok=True)
        with open(pdf_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        text = extract_text_from_pdf(pdf_path)
        metadata = {"filename": uploaded_file.name}
        doc_id = str(uuid.uuid4())
        store_document_in_qdrant(doc_id, text, metadata)
        st.success(f"âœ… TÃ i liá»‡u `{uploaded_file.name}` Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trá»¯ thÃ nh cÃ´ng!")

    query = st.text_input("â“ Nháº­p cÃ¢u há»i cá»§a báº¡n")
    if st.button("ğŸ” Tra cá»©u"):
        if query:
            results = search_documents(query)
            relevant_found = False
            relevant_results = []
            
            if results:
                for result in results:
                    filename = result.payload.get("filename", "KhÃ´ng cÃ³ tÃªn tÃ i liá»‡u")
                    full_text = result.payload.get("text_content", "")
                    similarity_score = result.score
                    relevant_text, page_number = extract_relevant_text(full_text, query, filename)
                    
                    if relevant_text:
                        is_relevant = check_relevance_with_gemini(query, relevant_text)
                        if is_relevant:
                            relevant_found = True
                            relevant_results.append({
                                "filename": filename,
                                "full_text": full_text,
                                "similarity_score": similarity_score,
                                "relevant_text": relevant_text,
                                "page_number": page_number
                            })
            
            if relevant_found:
                for res in relevant_results:
                    filename = res["filename"]
                    full_text = res["full_text"]
                    similarity_score = res["similarity_score"]
                    relevant_text = res["relevant_text"]
                    page_number = res["page_number"]
                    
                    pdf_path = f"uploads/{filename}"
                    if os.path.exists(pdf_path):
                        with open(pdf_path, "rb") as file:
                            pdf_data = file.read()
                        st.markdown(
                            f"ğŸ“„ **TÃ i liá»‡u:** {filename} - ğŸ“Œ **Trang:** {page_number} - "
                            f"ğŸ“Š **Äiá»ƒm tÆ°Æ¡ng Ä‘á»“ng:** {similarity_score:.4f}"
                        )
                        st.download_button(
                            label="ğŸ“¥ Táº£i xuá»‘ng tÃ i liá»‡u",
                            data=pdf_data,
                            file_name=filename,
                            mime="application/pdf"
                        )
                    else:
                        st.markdown(
                            f"ğŸ“„ **TÃ i liá»‡u:** {filename} (File khÃ´ng kháº£ dá»¥ng) - ğŸ“Œ **Trang:** {page_number} - "
                            f"ğŸ“Š **Äiá»ƒm tÆ°Æ¡ng Ä‘á»“ng:** {similarity_score:.4f}"
                        )
                    
                    st.markdown(f"ğŸ“Œ **Ná»™i dung trÃ­ch xuáº¥t ban Ä‘áº§u:**\n\n{relevant_text}")
                    # Truyá»n cáº£ full_text Ä‘á»ƒ Gemini cÃ³ thá»ƒ tÃ¬m kiáº¿m thÃªm náº¿u cáº§n
                    refined_answer = process_with_gemini(query, filename, relevant_text, full_text)
                    st.markdown(f"ğŸ¤– **Tráº£ lá»i tá»« Gemini:**\n\n{refined_answer}")
            else:
                refined_answer = process_with_gemini(query)
                st.markdown(f"ğŸ¤– **Tráº£ lá»i tá»« Gemini:**\n\n{refined_answer}")
        else:
            st.error("âŒ Vui lÃ²ng nháº­p cÃ¢u há»i!")

if __name__ == "__main__":
    main()