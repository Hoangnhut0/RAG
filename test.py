import streamlit as st
import fitz  # PyMuPDF Ä‘á»ƒ trÃ­ch xuáº¥t vÄƒn báº£n tá»« PDF
import pytesseract
from pdf2image import convert_from_path
import qdrant_client
from qdrant_client.models import PointStruct, Distance, VectorParams
import os
import uuid
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
import cv2  # OpenCV Ä‘á»ƒ xá»­ lÃ½ áº£nh
from PIL import Image  # Pillow Ä‘á»ƒ xá»­ lÃ½ áº£nh
from fuzzywuzzy import fuzz  # So sÃ¡nh chuá»—i gáº§n Ä‘Ãºng
import nltk  # Thay tháº¿ spaCy Ä‘á»ƒ xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn
import numpy as np

# Táº£i tÃ i nguyÃªn punkt_tab náº¿u chÆ°a cÃ³
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)

# ğŸ”¹ Cáº¥u hÃ¬nh Gemini API
genai.configure(api_key="AIzaSyCc5UqNQ1IrAN5rwLQQc4WTckKV0KRfgKw")

# ğŸ”¹ Khá»Ÿi táº¡o mÃ´ hÃ¬nh embedding
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# ğŸ”¹ Káº¿t ná»‘i Ä‘áº¿n Qdrant
qdrant = qdrant_client.QdrantClient("localhost", port=6333)
collection_name = "library_docs"

def ensure_collection_exists():
    collections = qdrant.get_collections()
    collection_names = [col.name for col in collections.collections]
    if collection_name not in collection_names:
        qdrant.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=384, distance=Distance.COSINE),
        )
        st.success(f"âœ… Collection `{collection_name}` Ä‘Ã£ Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng!")

ensure_collection_exists()

# ğŸ“„ Tiá»n xá»­ lÃ½ hÃ¬nh áº£nh trÆ°á»›c OCR
def preprocess_image(image):
    # Chuyá»ƒn sang grayscale
    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)
    # TÄƒng Ä‘á»™ tÆ°Æ¡ng pháº£n
    gray = cv2.convertScaleAbs(gray, alpha=1.5, beta=0)
    # Lá»c nhiá»…u
    gray = cv2.fastNlMeansDenoising(gray, h=10)
    return Image.fromarray(gray)

#  Sá»­a lá»—i chÃ­nh táº£ báº±ng Gemini AI
def correct_spelling_with_gemini(text):
    model = genai.GenerativeModel("gemini-2.0-flash")
    prompt = f"""
    VÄƒn báº£n sau cÃ³ thá»ƒ chá»©a lá»—i chÃ­nh táº£ tiáº¿ng Viá»‡t vÃ  tiáº¿ng Anh do trÃ­ch xuáº¥t tá»« OCR.
    HÃ£y sá»­a lá»—i chÃ­nh táº£ vÃ  tráº£ vá» vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c chá»‰nh sá»­a:
    "{text}"
    """
    try:
        response = model.generate_content(prompt)
        corrected_text = response.text.strip() if response else text
        return corrected_text
    except Exception as e:
        print(f"âŒ Lá»—i khi sá»­a chÃ­nh táº£ vá»›i Gemini: {e}")
        return text  

# ğŸ“„ TrÃ­ch xuáº¥t vÄƒn báº£n tá»« PDF thÆ°á»ng vÃ  PDF scan
def extract_text_from_pdf(pdf_path):
    try:
        doc = fitz.open(pdf_path)
        text_content = []
        
        for page_num, page in enumerate(doc, start=1):
            text = page.get_text("text").strip()
            if text:
                text_content.append(f"{text}\n[Trang {page_num}]")
        
        if text_content:
            extracted_text = "\n".join(text_content)
            #print(f"ğŸ“œ Ná»™i dung trÃ­ch xuáº¥t tá»« '{pdf_path}':\n{extracted_text}\n{'='*50}")
            return extracted_text
        
        # Náº¿u khÃ´ng cÃ³ vÄƒn báº£n -> Xá»­ lÃ½ báº±ng OCR
        return extract_text_with_ocr(pdf_path)
    except Exception as e:
        st.error(f"âŒ Lá»—i khi Ä‘á»c PDF: {e}")
        return "KhÃ´ng cÃ³ ná»™i dung."

#  Sá»­ dá»¥ng OCR vá»›i tiá»n xá»­ lÃ½ nÃ¢ng cao vÃ  sá»­a lá»—i chÃ­nh táº£
def extract_text_with_ocr(pdf_path):
    try:
        images = convert_from_path(pdf_path)
        text_content = []
        
        for i, img in enumerate(images):
            # Tiá»n xá»­ lÃ½ hÃ¬nh áº£nh
            processed_img = preprocess_image(img)
            # OCR vá»›i cáº¥u hÃ¬nh tÃ¹y chá»‰nh
            custom_config = r'--oem 3 --psm 6 -l eng+vie'
            raw_text = pytesseract.image_to_string(processed_img, config=custom_config).strip()
            if raw_text:
                # Sá»­a lá»—i chÃ­nh táº£
                corrected_text = correct_spelling_with_gemini(raw_text)
                text_content.append(f"{corrected_text}\n[Trang {i+1}]")
                # In vÄƒn báº£n thÃ´ vÃ  vÄƒn báº£n Ä‘Ã£ sá»­a ra terminal Ä‘á»ƒ kiá»ƒm tra
                #print(f"ğŸ“œ VÄƒn báº£n OCR thÃ´ (Trang {i+1}) tá»« '{pdf_path}':\n{raw_text}\n{'-'*50}")
                print(f"ğŸ“œ VÄƒn báº£n sau khi sá»­a lá»—i chÃ­nh táº£ (Trang {i+1}):\n{corrected_text}\n{'='*50}")
        
        extracted_text = "\n".join(text_content) if text_content else "KhÃ´ng cÃ³ ná»™i dung."
        return extracted_text
    except Exception as e:
        st.error(f"âŒ Lá»—i khi xá»­ lÃ½ OCR: {e}")
        return "KhÃ´ng cÃ³ ná»™i dung."

# ğŸ”¹ Chuyá»ƒn vÄƒn báº£n thÃ nh vector embedding
def generate_embedding(text):
    return embedding_model.encode(text).tolist() if text.strip() else [0.0] * 384

# ğŸ”¹ LÆ°u tÃ i liá»‡u vÃ o Qdrant
def store_document_in_qdrant(doc_id, text, metadata):
    vector = generate_embedding(text)
    metadata["text_content"] = text
    print(f"ğŸ“¦ VÄƒn báº£n cuá»‘i cÃ¹ng lÆ°u vÃ o Qdrant:\n{text}\n{'='*50}")  # In vÄƒn báº£n trÆ°á»›c khi lÆ°u
    qdrant.upsert(
        collection_name=collection_name,
        points=[PointStruct(id=doc_id, vector=vector, payload=metadata)]
    )

# ğŸ” TÃ¬m kiáº¿m tÃ i liá»‡u trong Qdrant vá»›i Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cáº£i thiá»‡n
def search_documents(query):
    query_vector = generate_embedding(query)
    results = qdrant.search(
        collection_name=collection_name,
        query_vector=query_vector,
        limit=10,
        with_payload=True
    )
    # Lá»c káº¿t quáº£ dá»±a trÃªn Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng chuá»—i
    filtered_results = []
    for r in results:
        text = r.payload.get("text_content", "").lower()
        if fuzz.partial_ratio(query.lower(), text) > 70 or r.score >= 0.4:
            filtered_results.append(r)
    return filtered_results

# ğŸ“Œ TrÃ­ch xuáº¥t Ä‘oáº¡n vÄƒn báº£n vá»›i nltk vÃ  tÃ¬m sá»‘ trang chÃ­nh xÃ¡c hÆ¡n
def extract_relevant_text(full_text, query, filename, context_size=300):
    query_lower = query.lower()
    sentences = nltk.sent_tokenize(full_text)  # TÃ¡ch cÃ¢u vá»›i nltk
    
    for sent in sentences:
        if query_lower in sent.lower():
            # TÃ¬m vá»‹ trÃ­ cá»§a cÃ¢u trong vÄƒn báº£n gá»‘c
            start_idx = full_text.lower().find(sent.lower())
            if start_idx == -1:
                continue
            start = max(0, start_idx - context_size // 2)
            end = min(len(full_text), start_idx + len(sent) + context_size // 2)
            relevant_text = full_text[start:end]
            
            # TÃ¬m sá»‘ trang gáº§n nháº¥t trÆ°á»›c hoáº·c sau Ä‘oáº¡n vÄƒn báº£n
            page_number = "KhÃ´ng rÃµ trang"
            text_before = full_text[:end]  # Láº¥y toÃ n bá»™ vÄƒn báº£n tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i Ä‘oáº¡n trÃ­ch xuáº¥t
            pages = [line for line in text_before.split("\n") if "[Trang " in line]
            if pages:
                # Láº¥y sá»‘ trang tá»« dÃ²ng cuá»‘i cÃ¹ng cÃ³ chá»©a "[Trang ...]"
                last_page_line = pages[-1]
                page_number = last_page_line.split("[Trang ")[-1].replace("]", "").strip()
            
            return f"{relevant_text}...", page_number
    return None, "KhÃ´ng rÃµ trang"

# ğŸ¤– Xá»­ lÃ½ Gemini AI
def process_with_gemini(query, filename=None, context=None):
    model = genai.GenerativeModel("gemini-2.0-flash")
    if context:
        prompt = f"""
        NgÆ°á»i dÃ¹ng Ä‘ang tÃ¬m kiáº¿m thÃ´ng tin trong tÃ i liá»‡u: `{filename}`
        Truy váº¥n: "{query}"
        Ná»™i dung liÃªn quan tá»« tÃ i liá»‡u:
        {context}
        HÃ£y diá»…n giáº£i láº¡i thÃ´ng tin má»™t cÃ¡ch dá»… hiá»ƒu vÃ  cung cáº¥p ná»™i dung tá»‘i Æ°u nháº¥t.
        """
    else:
        prompt = f'NgÆ°á»i dÃ¹ng Ä‘ang há»i: "{query}" KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u phÃ¹ há»£p.'
    response = model.generate_content(prompt)
    return response.text if response else "KhÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i."

# ğŸ¯ Giao diá»‡n Streamlit
def main():
    st.title("ğŸ“š ThÆ° viá»‡n sá»‘ thÃ´ng minh")
    
    uploaded_file = st.file_uploader("ğŸ“¤ Táº£i lÃªn tÃ i liá»‡u PDF", type=["pdf"])
    if uploaded_file and st.button("ğŸ“¥ LÆ°u tÃ i liá»‡u"):
        pdf_path = f"uploads/{uploaded_file.name}"
        os.makedirs("uploads", exist_ok=True)
        with open(pdf_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        text = extract_text_from_pdf(pdf_path)
        metadata = {"filename": uploaded_file.name}
        doc_id = str(uuid.uuid4())
        store_document_in_qdrant(doc_id, text, metadata)
        st.success(f"âœ… TÃ i liá»‡u `{uploaded_file.name}` Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trá»¯ thÃ nh cÃ´ng!")

    query = st.text_input("ğŸ” Nháº­p tá»« khÃ³a tÃ¬m kiáº¿m")
    if st.button("ğŸ” TÃ¬m kiáº¿m"):
        results = search_documents(query)
        
        if results:
            for result in results:
                filename = result.payload.get("filename", "KhÃ´ng cÃ³ tÃªn tÃ i liá»‡u")
                full_text = result.payload.get("text_content", "")
                relevant_text, page_number = extract_relevant_text(full_text, query, filename)
                
                if relevant_text:
                    st.markdown(f"ğŸ“„ **TÃ i liá»‡u:** {filename} - ğŸ“Œ **Trang:** {page_number}")
                    st.markdown(f"ğŸ“Œ **Ná»™i dung trÃ­ch xuáº¥t:**\n\n{relevant_text}")
                    refined_answer = process_with_gemini(query, filename, relevant_text)
                    st.markdown(f"ğŸ¤– **Tráº£ lá»i:**\n\n{refined_answer}")

if __name__ == "__main__":
    main()  